{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5556a683",
   "metadata": {},
   "source": [
    "## Open Dataset\n",
    "Import Pandas and import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d03ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_sentence</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>Label_bias</th>\n",
       "      <th>biased_words4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube is making clear there will be no “birt...</td>\n",
       "      <td>usa-today</td>\n",
       "      <td>elections-2020</td>\n",
       "      <td>center</td>\n",
       "      <td>Biased</td>\n",
       "      <td>['belated', 'birtherism']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The increasingly bitter dispute between Americ...</td>\n",
       "      <td>msnbc</td>\n",
       "      <td>sport</td>\n",
       "      <td>left</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>['bitter']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So while there may be a humanitarian crisis dr...</td>\n",
       "      <td>alternet</td>\n",
       "      <td>immigration</td>\n",
       "      <td>left</td>\n",
       "      <td>Biased</td>\n",
       "      <td>['crisis']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>breitbart</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>['legitimate']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Looking around the United States, there is nev...</td>\n",
       "      <td>federalist</td>\n",
       "      <td>abortion</td>\n",
       "      <td>right</td>\n",
       "      <td>Biased</td>\n",
       "      <td>['killing', 'never', 'developing', 'humans', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     origin_sentence      outlet  \\\n",
       "0  YouTube is making clear there will be no “birt...   usa-today   \n",
       "1  The increasingly bitter dispute between Americ...       msnbc   \n",
       "2  So while there may be a humanitarian crisis dr...    alternet   \n",
       "3  A professor who teaches climate change classes...   breitbart   \n",
       "4  Looking around the United States, there is nev...  federalist   \n",
       "\n",
       "            topic    type  Label_bias  \\\n",
       "0  elections-2020  center      Biased   \n",
       "1           sport    left  Non-biased   \n",
       "2     immigration    left      Biased   \n",
       "3     environment   right  Non-biased   \n",
       "4        abortion   right      Biased   \n",
       "\n",
       "                                       biased_words4  \n",
       "0                          ['belated', 'birtherism']  \n",
       "1                                         ['bitter']  \n",
       "2                                         ['crisis']  \n",
       "3                                     ['legitimate']  \n",
       "4  ['killing', 'never', 'developing', 'humans', '...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from joblib import dump, load\n",
    "\n",
    "dataset = pandas.read_csv('dataset.csv')\n",
    "#Remove non-important features\n",
    "dataset = dataset.drop(['Unnamed: 0', 'news_link', 'group_id', 'num_sent', 'article', 'Label_opinion'], axis=1)\n",
    "dataset.rename(columns = {'sentence':'origin_sentence'}, inplace = True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3b249",
   "metadata": {},
   "source": [
    "## Remove No agreement values & Convert Label to Binary\n",
    "1 is Biased, 0 is Non-Biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d716132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>origin_sentence</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>Label_bias</th>\n",
       "      <th>biased_words4</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>YouTube is making clear there will be no “birt...</td>\n",
       "      <td>usa-today</td>\n",
       "      <td>elections-2020</td>\n",
       "      <td>center</td>\n",
       "      <td>Biased</td>\n",
       "      <td>['belated', 'birtherism']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The increasingly bitter dispute between Americ...</td>\n",
       "      <td>msnbc</td>\n",
       "      <td>sport</td>\n",
       "      <td>left</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>['bitter']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>So while there may be a humanitarian crisis dr...</td>\n",
       "      <td>alternet</td>\n",
       "      <td>immigration</td>\n",
       "      <td>left</td>\n",
       "      <td>Biased</td>\n",
       "      <td>['crisis']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>breitbart</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>['legitimate']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Looking around the United States, there is nev...</td>\n",
       "      <td>federalist</td>\n",
       "      <td>abortion</td>\n",
       "      <td>right</td>\n",
       "      <td>Biased</td>\n",
       "      <td>['killing', 'never', 'developing', 'humans', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                    origin_sentence      outlet  \\\n",
       "0      0  YouTube is making clear there will be no “birt...   usa-today   \n",
       "1      1  The increasingly bitter dispute between Americ...       msnbc   \n",
       "2      2  So while there may be a humanitarian crisis dr...    alternet   \n",
       "3      3  A professor who teaches climate change classes...   breitbart   \n",
       "4      4  Looking around the United States, there is nev...  federalist   \n",
       "\n",
       "            topic    type  Label_bias  \\\n",
       "0  elections-2020  center      Biased   \n",
       "1           sport    left  Non-biased   \n",
       "2     immigration    left      Biased   \n",
       "3     environment   right  Non-biased   \n",
       "4        abortion   right      Biased   \n",
       "\n",
       "                                       biased_words4  label  \n",
       "0                          ['belated', 'birtherism']      1  \n",
       "1                                         ['bitter']      0  \n",
       "2                                         ['crisis']      1  \n",
       "3                                     ['legitimate']      0  \n",
       "4  ['killing', 'never', 'developing', 'humans', '...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[dataset['Label_bias']!=\"No agreement\"].reset_index()\n",
    "dataset['label'] = [1 if val==\"Biased\" else 0 for val in dataset['Label_bias']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3044b8",
   "metadata": {},
   "source": [
    "## Prepare set with sentence metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d6d60ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>label_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   outlet  topic  type  label_bias\n",
       "0       7      2     0           1\n",
       "1       5      9     1           0\n",
       "2       0      6     1           1\n",
       "3       1      3     2           0\n",
       "4       2      0     2           1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "outlets = preprocessing.LabelEncoder().fit(dataset['outlet'])\n",
    "topics = preprocessing.LabelEncoder().fit(dataset['topic'])\n",
    "types = preprocessing.LabelEncoder().fit(dataset['type'])\n",
    "\n",
    "metaset = pandas.DataFrame(data={'outlet': outlets.transform(dataset['outlet']),\n",
    "                                 'topic': topics.transform(dataset['topic']),\n",
    "                                 'type': types.transform(dataset['type']),\n",
    "                                 'label_bias': dataset['label']})\n",
    "\n",
    "dump(metaset, 'Joblibs/metaset.joblib')\n",
    "\n",
    "metaset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f678ac",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "Prepare for stemming and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd7c8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "excluded = ['.', ',', '“', '”', '’', '–', '—', '―', '(', ')', ';', '--', '``', \"''\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6634210c",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Use snowball stemmer and generate a TF-IDF Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825bdd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_sentence</th>\n",
       "      <th>youtub</th>\n",
       "      <th>make</th>\n",
       "      <th>clear</th>\n",
       "      <th>birther</th>\n",
       "      <th>platform</th>\n",
       "      <th>year</th>\n",
       "      <th>u.s.</th>\n",
       "      <th>presidenti</th>\n",
       "      <th>elect</th>\n",
       "      <th>...</th>\n",
       "      <th>lord</th>\n",
       "      <th>rob</th>\n",
       "      <th>hesit</th>\n",
       "      <th>hysteria</th>\n",
       "      <th>ratif</th>\n",
       "      <th>rodney</th>\n",
       "      <th>haaland</th>\n",
       "      <th>self-describ</th>\n",
       "      <th>bemoan</th>\n",
       "      <th>label_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube is making clear there will be no “birt...</td>\n",
       "      <td>0.317691</td>\n",
       "      <td>0.168119</td>\n",
       "      <td>0.238283</td>\n",
       "      <td>0.432156</td>\n",
       "      <td>0.291104</td>\n",
       "      <td>0.159525</td>\n",
       "      <td>0.149087</td>\n",
       "      <td>0.205677</td>\n",
       "      <td>0.191312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The increasingly bitter dispute between Americ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So while there may be a humanitarian crisis dr...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Looking around the United States, there is nev...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>In every case legislators are being swarmed by...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408148</td>\n",
       "      <td>0.408148</td>\n",
       "      <td>0.408148</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>Polls show the transgender ideology is deeply ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>Democrats and Republicans stood and applauded ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>As a self-described Democratic socialist, Sen....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.667878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>During the segment, Colbert also bemoaned the ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1551 rows × 6300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        origin_sentence    youtub      make  \\\n",
       "0     YouTube is making clear there will be no “birt...  0.317691  0.168119   \n",
       "1     The increasingly bitter dispute between Americ...  0.000000  0.000000   \n",
       "2     So while there may be a humanitarian crisis dr...  0.000000  0.000000   \n",
       "3     A professor who teaches climate change classes...  0.000000  0.000000   \n",
       "4     Looking around the United States, there is nev...  0.000000  0.000000   \n",
       "...                                                 ...       ...       ...   \n",
       "1546  In every case legislators are being swarmed by...  0.000000  0.000000   \n",
       "1547  Polls show the transgender ideology is deeply ...  0.000000  0.000000   \n",
       "1548  Democrats and Republicans stood and applauded ...  0.000000  0.124262   \n",
       "1549  As a self-described Democratic socialist, Sen....  0.000000  0.000000   \n",
       "1550  During the segment, Colbert also bemoaned the ...  0.000000  0.000000   \n",
       "\n",
       "         clear   birther  platform      year      u.s.  presidenti     elect  \\\n",
       "0     0.238283  0.432156  0.291104  0.159525  0.149087    0.205677  0.191312   \n",
       "1     0.000000  0.000000  0.000000  0.000000  0.093869    0.000000  0.000000   \n",
       "2     0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "3     0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "...        ...       ...       ...       ...       ...         ...       ...   \n",
       "1546  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "1547  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "1548  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "1549  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "1550  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "\n",
       "      ...  lord  rob     hesit  hysteria     ratif   rodney  haaland  \\\n",
       "0     ...   0.0  0.0  0.000000  0.000000  0.000000  0.00000  0.00000   \n",
       "1     ...   0.0  0.0  0.000000  0.000000  0.000000  0.00000  0.00000   \n",
       "2     ...   0.0  0.0  0.000000  0.000000  0.000000  0.00000  0.00000   \n",
       "3     ...   0.0  0.0  0.000000  0.000000  0.000000  0.00000  0.00000   \n",
       "4     ...   0.0  0.0  0.000000  0.000000  0.000000  0.00000  0.00000   \n",
       "...   ...   ...  ...       ...       ...       ...      ...      ...   \n",
       "1546  ...   0.0  0.0  0.408148  0.408148  0.408148  0.00000  0.00000   \n",
       "1547  ...   0.0  0.0  0.000000  0.000000  0.000000  0.00000  0.00000   \n",
       "1548  ...   0.0  0.0  0.000000  0.000000  0.000000  0.31942  0.31942   \n",
       "1549  ...   0.0  0.0  0.000000  0.000000  0.000000  0.00000  0.00000   \n",
       "1550  ...   0.0  0.0  0.000000  0.000000  0.000000  0.00000  0.00000   \n",
       "\n",
       "      self-describ    bemoan  label_bias  \n",
       "0         0.000000  0.000000           1  \n",
       "1         0.000000  0.000000           0  \n",
       "2         0.000000  0.000000           1  \n",
       "3         0.000000  0.000000           0  \n",
       "4         0.000000  0.000000           1  \n",
       "...            ...       ...         ...  \n",
       "1546      0.000000  0.000000           1  \n",
       "1547      0.000000  0.000000           1  \n",
       "1548      0.000000  0.000000           0  \n",
       "1549      0.667878  0.000000           0  \n",
       "1550      0.000000  0.734666           0  \n",
       "\n",
       "[1551 rows x 6300 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stem_word_matrix = pandas.DataFrame(data=dataset['origin_sentence'])\n",
    "total_terms = {}\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "#Generate DataFrame With TF Confusion Matrix\n",
    "row = 0\n",
    "for sentence in stem_word_matrix['origin_sentence']:\n",
    "    words = word_tokenize(sentence)\n",
    "    word_num = 0\n",
    "    doc_terms = {}\n",
    "    terms_found = []\n",
    "    for word in words:\n",
    "        if word not in stopwords and word not in excluded:\n",
    "            word_stem = stemmer.stem(word)\n",
    "            word_num += 1\n",
    "            word = word_stem.lower()\n",
    "            if word in doc_terms:\n",
    "                doc_terms[word] += 1\n",
    "            else:\n",
    "                doc_terms[word] = 1\n",
    "            if word not in terms_found:\n",
    "                terms_found.append(word)\n",
    "                if word in total_terms:\n",
    "                    total_terms[word] += 1\n",
    "                else:\n",
    "                    total_terms[word] = 1\n",
    "    for word in doc_terms.keys():\n",
    "        stem_word_matrix.at[row, word] = doc_terms[word]/word_num\n",
    "    row += 1\n",
    "stem_word_matrix = stem_word_matrix.fillna(0)\n",
    "\n",
    "#Calculate IDF\n",
    "for term in total_terms.keys():\n",
    "    stem_word_matrix[term] *= numpy.log(stem_word_matrix['origin_sentence'].count() / total_terms[term])\n",
    "    \n",
    "stem_word_matrix['label_bias'] = dataset['label']\n",
    "\n",
    "dump(stem_word_matrix, 'Joblibs/stem_word_matrix.joblib')\n",
    "\n",
    "stem_word_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b41c64",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "Use WordNetLemmatizer and generate a TF-IDF Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34102176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_sentence</th>\n",
       "      <th>youtube</th>\n",
       "      <th>making</th>\n",
       "      <th>clear</th>\n",
       "      <th>birtherism</th>\n",
       "      <th>platform</th>\n",
       "      <th>year</th>\n",
       "      <th>u.s.</th>\n",
       "      <th>presidential</th>\n",
       "      <th>election</th>\n",
       "      <th>...</th>\n",
       "      <th>happening</th>\n",
       "      <th>polls</th>\n",
       "      <th>rodney</th>\n",
       "      <th>davis</th>\n",
       "      <th>saluted</th>\n",
       "      <th>haaland</th>\n",
       "      <th>preside</th>\n",
       "      <th>self-described</th>\n",
       "      <th>bemoaned</th>\n",
       "      <th>label_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube is making clear there will be no “birt...</td>\n",
       "      <td>0.317691</td>\n",
       "      <td>0.232086</td>\n",
       "      <td>0.265497</td>\n",
       "      <td>0.432156</td>\n",
       "      <td>0.291104</td>\n",
       "      <td>0.160099</td>\n",
       "      <td>0.149087</td>\n",
       "      <td>0.205677</td>\n",
       "      <td>0.204438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The increasingly bitter dispute between Americ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So while there may be a humanitarian crisis dr...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Looking around the United States, there is nev...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>In every case legislators are being swarmed by...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>Polls show the transgender ideology is deeply ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734666</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>Democrats and Republicans stood and applauded ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>As a self-described Democratic socialist, Sen....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.667878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>During the segment, Colbert also bemoaned the ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1551 rows × 8093 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        origin_sentence   youtube    making  \\\n",
       "0     YouTube is making clear there will be no “birt...  0.317691  0.232086   \n",
       "1     The increasingly bitter dispute between Americ...  0.000000  0.000000   \n",
       "2     So while there may be a humanitarian crisis dr...  0.000000  0.000000   \n",
       "3     A professor who teaches climate change classes...  0.000000  0.000000   \n",
       "4     Looking around the United States, there is nev...  0.000000  0.000000   \n",
       "...                                                 ...       ...       ...   \n",
       "1546  In every case legislators are being swarmed by...  0.000000  0.000000   \n",
       "1547  Polls show the transgender ideology is deeply ...  0.000000  0.000000   \n",
       "1548  Democrats and Republicans stood and applauded ...  0.000000  0.171542   \n",
       "1549  As a self-described Democratic socialist, Sen....  0.000000  0.000000   \n",
       "1550  During the segment, Colbert also bemoaned the ...  0.000000  0.000000   \n",
       "\n",
       "         clear  birtherism  platform      year      u.s.  presidential  \\\n",
       "0     0.265497    0.432156  0.291104  0.160099  0.149087      0.205677   \n",
       "1     0.000000    0.000000  0.000000  0.000000  0.093869      0.000000   \n",
       "2     0.000000    0.000000  0.000000  0.000000  0.000000      0.000000   \n",
       "3     0.000000    0.000000  0.000000  0.000000  0.000000      0.000000   \n",
       "4     0.000000    0.000000  0.000000  0.000000  0.000000      0.000000   \n",
       "...        ...         ...       ...       ...       ...           ...   \n",
       "1546  0.000000    0.000000  0.000000  0.000000  0.000000      0.000000   \n",
       "1547  0.000000    0.000000  0.000000  0.000000  0.000000      0.000000   \n",
       "1548  0.000000    0.000000  0.000000  0.000000  0.000000      0.000000   \n",
       "1549  0.000000    0.000000  0.000000  0.000000  0.000000      0.000000   \n",
       "1550  0.000000    0.000000  0.000000  0.000000  0.000000      0.000000   \n",
       "\n",
       "      election  ...  happening     polls   rodney    davis  saluted  haaland  \\\n",
       "0     0.204438  ...   0.000000  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "1     0.000000  ...   0.000000  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "2     0.000000  ...   0.000000  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "3     0.000000  ...   0.000000  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "4     0.000000  ...   0.000000  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "...        ...  ...        ...       ...      ...      ...      ...      ...   \n",
       "1546  0.000000  ...   0.408148  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "1547  0.000000  ...   0.000000  0.734666  0.00000  0.00000  0.00000  0.00000   \n",
       "1548  0.000000  ...   0.000000  0.000000  0.31942  0.31942  0.31942  0.31942   \n",
       "1549  0.000000  ...   0.000000  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "1550  0.000000  ...   0.000000  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "\n",
       "      preside  self-described  bemoaned  label_bias  \n",
       "0     0.00000        0.000000  0.000000           1  \n",
       "1     0.00000        0.000000  0.000000           0  \n",
       "2     0.00000        0.000000  0.000000           1  \n",
       "3     0.00000        0.000000  0.000000           0  \n",
       "4     0.00000        0.000000  0.000000           1  \n",
       "...       ...             ...       ...         ...  \n",
       "1546  0.00000        0.000000  0.000000           1  \n",
       "1547  0.00000        0.000000  0.000000           1  \n",
       "1548  0.31942        0.000000  0.000000           0  \n",
       "1549  0.00000        0.667878  0.000000           0  \n",
       "1550  0.00000        0.000000  0.734666           0  \n",
       "\n",
       "[1551 rows x 8093 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem_word_matrix = pandas.DataFrame(data=dataset['origin_sentence'])\n",
    "total_terms = {}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "row = 0\n",
    "for sentence in lem_word_matrix['origin_sentence']:\n",
    "    words = word_tokenize(sentence)\n",
    "    word_num = 0\n",
    "    doc_terms = {}\n",
    "    terms_found = []\n",
    "    for word in words:\n",
    "        if word not in stopwords and word not in excluded:\n",
    "            word_lem = lemmatizer.lemmatize(word)\n",
    "            word_num += 1\n",
    "            word = word_lem.lower()\n",
    "            if word in doc_terms:\n",
    "                doc_terms[word] += 1\n",
    "            else:\n",
    "                doc_terms[word] = 1\n",
    "            if word not in terms_found:\n",
    "                terms_found.append(word)\n",
    "                if word in total_terms:\n",
    "                    total_terms[word] += 1\n",
    "                else:\n",
    "                    total_terms[word] = 1\n",
    "    for word in doc_terms.keys():\n",
    "        lem_word_matrix.at[row, word] = doc_terms[word]/word_num\n",
    "    row += 1\n",
    "lem_word_matrix = lem_word_matrix.fillna(0)\n",
    "\n",
    "#Calculate IDF\n",
    "for term in total_terms.keys():\n",
    "        lem_word_matrix[term] *= numpy.log(lem_word_matrix['origin_sentence'].count() / total_terms[term])\n",
    "\n",
    "lem_word_matrix['label_bias'] = dataset['label']\n",
    "\n",
    "dump(lem_word_matrix, 'Joblibs/lem_word_matrix.joblib')\n",
    "\n",
    "lem_word_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b8c77",
   "metadata": {},
   "source": [
    "## Create metrics and split datasets as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ca8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro']\n",
    "\n",
    "try:\n",
    "    metaset\n",
    "    stem_word_matrix\n",
    "    lem_word_matrix\n",
    "except NameError:\n",
    "    metaset = load('Joblibs/metaset.joblib')\n",
    "    stem_word_matrix = load('Joblibs/stem_word_matrix.joblib')\n",
    "    lem_word_matrix = load('Joblibs/lem_word_matrix.joblib')\n",
    "\n",
    "metaset_data = metaset.drop(['label_bias'], axis=1)\n",
    "metaset_label = metaset['label_bias']\n",
    "stem_word_matrix_data = stem_word_matrix.drop(['label_bias', 'origin_sentence'], axis=1)\n",
    "stem_word_matrix_label = stem_word_matrix['label_bias']\n",
    "lem_word_matrix_data = lem_word_matrix.drop(['label_bias', 'origin_sentence'], axis=1)\n",
    "lem_word_matrix_label = lem_word_matrix['label_bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d4731",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea2f2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': 0.0032447099685668944,\n",
       " 'score_time': 0.004186248779296875,\n",
       " 'test_precision_macro': 0.7217190424374867,\n",
       " 'test_recall_macro': 0.6476015933076844}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Parameters\n",
    "dt = DecisionTreeClassifier(criterion='gini', # gini | entropy\n",
    "                            splitter='best', # best | random\n",
    "                            max_depth=5, # int | None\n",
    "                            max_features='auto') # auto | sqrt | log2 | None \n",
    "\n",
    "scores = cross_validate(dt, metaset_data, metaset_label, scoring=scoring, cv=10)\n",
    "dt_scores = {}\n",
    "for score in scores:\n",
    "    summ = 0\n",
    "    for value in scores[score]:\n",
    "        summ += value\n",
    "    dt_scores[score] = summ / 10\n",
    "\n",
    "dump(dt, 'Joblibs/decision_trees.joblib')\n",
    "    \n",
    "dt_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefada8e",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196ccb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': 0.005458831787109375,\n",
       " 'score_time': 0.26069231033325196,\n",
       " 'test_precision_macro': 0.7209633209334845,\n",
       " 'test_recall_macro': 0.6926802086423959}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Parameters\n",
    "knns = KNeighborsClassifier(n_neighbors=20, # int\n",
    "                            weights='uniform', # uniform | distance\n",
    "                            algorithm='ball_tree', # auto | ball_tree | kd_tree | brute\n",
    "                            leaf_size=30, # int\n",
    "                            p=2, # manhattan_dist (1) | euclidean_dist (2) | minkowski_dist(3)\n",
    "                            n_jobs=2) # threads\n",
    "\n",
    "scores = cross_validate(knns, metaset_data, metaset_label.values.ravel(), scoring=scoring, cv=10)\n",
    "knn_scores = {}\n",
    "for score in scores:\n",
    "    summ = 0\n",
    "    for value in scores[score]:\n",
    "        summ += value\n",
    "    knn_scores[score] = summ / 10\n",
    "    \n",
    "dump(knns, 'Joblibs/k-nearest_neighbors.joblib')\n",
    "\n",
    "knn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f995085",
   "metadata": {},
   "source": [
    "## Gradient Boosted Decision Trees\n",
    "\n",
    "### With Stemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88972e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': 7.846212577819824,\n",
       " 'score_time': 0.059369778633117674,\n",
       " 'test_precision_macro': 0.7142249552756057,\n",
       " 'test_recall_macro': 0.5478499524018254}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Parameters\n",
    "gbdt_stem = GradientBoostingClassifier(loss='deviance', # deviance | exponential\n",
    "                                       learning_rate=0.01, # float\n",
    "                                       n_estimators=80, # int\n",
    "                                       subsample=0.4, # float\n",
    "                                       max_depth=8) # int\n",
    "\n",
    "scores = cross_validate(gbdt_stem, stem_word_matrix_data, stem_word_matrix_label, scoring=scoring, cv=10)\n",
    "gbdt_stem_scores = {}\n",
    "for score in scores:\n",
    "    summ = 0\n",
    "    for value in scores[score]:\n",
    "        summ += value\n",
    "    gbdt_stem_scores[score] = summ / 10\n",
    "\n",
    "dump(gbdt_stem, 'Joblibs/gradient_boosted_stem.joblib')\n",
    "    \n",
    "gbdt_stem_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a9b5ff",
   "metadata": {},
   "source": [
    "### With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23bca50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': 3.2826137065887453,\n",
       " 'score_time': 0.07647171020507812,\n",
       " 'test_precision_macro': 0.7223479750787789,\n",
       " 'test_recall_macro': 0.527427629000414}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Parameters\n",
    "gbdt_lem = GradientBoostingClassifier(loss='exponential', # deviance | exponential\n",
    "                                      learning_rate=0.01, # float\n",
    "                                      n_estimators=80, # int\n",
    "                                      subsample=0.4, # float\n",
    "                                      max_depth=2) # int\n",
    "\n",
    "scores = cross_validate(gbdt_lem, lem_word_matrix_data, lem_word_matrix_label, scoring=scoring, cv=10)\n",
    "gbdt_lem_scores = {}\n",
    "for score in scores:\n",
    "    summ = 0\n",
    "    for value in scores[score]:\n",
    "        summ += value\n",
    "    gbdt_lem_scores[score] = summ / 10\n",
    "\n",
    "dump(gbdt_lem, 'Joblibs/gradient_boosted_lem.joblib')\n",
    "    \n",
    "gbdt_lem_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5963d426",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "### With Stemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ccb4b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': 93.04257156848908,\n",
       " 'score_time': 0.12363924980163574,\n",
       " 'test_precision_macro': 0.6995379831207054,\n",
       " 'test_recall_macro': 0.6701313111333331}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Parameters\n",
    "mlp_stem = MLPClassifier(hidden_layer_sizes=(200, 100, 50), # tuple | int\n",
    "                         activation='logistic', # identity | logistic | tanh | relu\n",
    "                         solver='adam', #lbfgs | sgd | adam\n",
    "                         learning_rate='adaptive', # constant | invscaling | adaptive\n",
    "                         max_iter=200) # int\n",
    "\n",
    "scores = cross_validate(mlp_stem, stem_word_matrix_data, stem_word_matrix_label, scoring=scoring, cv=10)\n",
    "mlp_stem_scores = {}\n",
    "for score in scores:\n",
    "    summ = 0\n",
    "    for value in scores[score]:\n",
    "        summ += value\n",
    "    mlp_stem_scores[score] = summ / 10\n",
    "\n",
    "dump(mlp_stem, 'Joblibs/multi_layer_perceptron_stem.joblib')\n",
    "    \n",
    "mlp_stem_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f928d53",
   "metadata": {},
   "source": [
    "### With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dcd4c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': 67.97297763824463,\n",
       " 'score_time': 0.24845488071441652,\n",
       " 'test_precision_macro': 0.6991661873328471,\n",
       " 'test_recall_macro': 0.6336320276496759}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Parameters\n",
    "mlp_lem = MLPClassifier(hidden_layer_sizes=(750, 500, 250, 100), # tuple | int\n",
    "                        activation='tanh', # identity | logistic | tanh | relu\n",
    "                        solver='adam', #lbfgs | sgd | adam\n",
    "                        learning_rate='adaptive', # constant | invscaling | adaptive\n",
    "                        max_iter=300) # int\n",
    "\n",
    "scores = cross_validate(mlp_lem, lem_word_matrix_data, lem_word_matrix_label, scoring=scoring, cv=10)\n",
    "mlp_lem_scores = {}\n",
    "for score in scores:\n",
    "    summ = 0\n",
    "    for value in scores[score]:\n",
    "        summ += value\n",
    "    mlp_lem_scores[score] = summ / 10\n",
    "\n",
    "dump(mlp_lem, 'Joblibs/multi_layer_perceptron_lem.joblib')    \n",
    "\n",
    "mlp_lem_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1fd41",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "### Create a dataset to rule them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bd1af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6302\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_sentence</th>\n",
       "      <th>meta_outlet</th>\n",
       "      <th>meta_topic</th>\n",
       "      <th>meta_type</th>\n",
       "      <th>stem_youtub</th>\n",
       "      <th>stem_make</th>\n",
       "      <th>stem_clear</th>\n",
       "      <th>stem_birther</th>\n",
       "      <th>stem_platform</th>\n",
       "      <th>stem_year</th>\n",
       "      <th>...</th>\n",
       "      <th>lem_happening</th>\n",
       "      <th>lem_polls</th>\n",
       "      <th>lem_rodney</th>\n",
       "      <th>lem_davis</th>\n",
       "      <th>lem_saluted</th>\n",
       "      <th>lem_haaland</th>\n",
       "      <th>lem_preside</th>\n",
       "      <th>lem_self-described</th>\n",
       "      <th>lem_bemoaned</th>\n",
       "      <th>label_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube is making clear there will be no “birt...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317691</td>\n",
       "      <td>0.168119</td>\n",
       "      <td>0.238283</td>\n",
       "      <td>0.432156</td>\n",
       "      <td>0.291104</td>\n",
       "      <td>0.159525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The increasingly bitter dispute between Americ...</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So while there may be a humanitarian crisis dr...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Looking around the United States, there is nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>In every case legislators are being swarmed by...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>Polls show the transgender ideology is deeply ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734666</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>Democrats and Republicans stood and applauded ...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.31942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>As a self-described Democratic socialist, Sen....</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.667878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>During the segment, Colbert also bemoaned the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1551 rows × 14394 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        origin_sentence  meta_outlet  \\\n",
       "0     YouTube is making clear there will be no “birt...            7   \n",
       "1     The increasingly bitter dispute between Americ...            5   \n",
       "2     So while there may be a humanitarian crisis dr...            0   \n",
       "3     A professor who teaches climate change classes...            1   \n",
       "4     Looking around the United States, there is nev...            2   \n",
       "...                                                 ...          ...   \n",
       "1546  In every case legislators are being swarmed by...            0   \n",
       "1547  Polls show the transgender ideology is deeply ...            1   \n",
       "1548  Democrats and Republicans stood and applauded ...            7   \n",
       "1549  As a self-described Democratic socialist, Sen....            3   \n",
       "1550  During the segment, Colbert also bemoaned the ...            1   \n",
       "\n",
       "      meta_topic  meta_type  stem_youtub  stem_make  stem_clear  stem_birther  \\\n",
       "0              2          0     0.317691   0.168119    0.238283      0.432156   \n",
       "1              9          1     0.000000   0.000000    0.000000      0.000000   \n",
       "2              6          1     0.000000   0.000000    0.000000      0.000000   \n",
       "3              3          2     0.000000   0.000000    0.000000      0.000000   \n",
       "4              0          2     0.000000   0.000000    0.000000      0.000000   \n",
       "...          ...        ...          ...        ...         ...           ...   \n",
       "1546           4          1     0.000000   0.000000    0.000000      0.000000   \n",
       "1547           4          2     0.000000   0.000000    0.000000      0.000000   \n",
       "1548           4          0     0.000000   0.124262    0.000000      0.000000   \n",
       "1549           8          2     0.000000   0.000000    0.000000      0.000000   \n",
       "1550          13          2     0.000000   0.000000    0.000000      0.000000   \n",
       "\n",
       "      stem_platform  stem_year  ...  lem_happening  lem_polls  lem_rodney  \\\n",
       "0          0.291104   0.159525  ...       0.000000   0.000000     0.00000   \n",
       "1          0.000000   0.000000  ...       0.000000   0.000000     0.00000   \n",
       "2          0.000000   0.000000  ...       0.000000   0.000000     0.00000   \n",
       "3          0.000000   0.000000  ...       0.000000   0.000000     0.00000   \n",
       "4          0.000000   0.000000  ...       0.000000   0.000000     0.00000   \n",
       "...             ...        ...  ...            ...        ...         ...   \n",
       "1546       0.000000   0.000000  ...       0.408148   0.000000     0.00000   \n",
       "1547       0.000000   0.000000  ...       0.000000   0.734666     0.00000   \n",
       "1548       0.000000   0.000000  ...       0.000000   0.000000     0.31942   \n",
       "1549       0.000000   0.000000  ...       0.000000   0.000000     0.00000   \n",
       "1550       0.000000   0.000000  ...       0.000000   0.000000     0.00000   \n",
       "\n",
       "      lem_davis  lem_saluted  lem_haaland  lem_preside  lem_self-described  \\\n",
       "0       0.00000      0.00000      0.00000      0.00000            0.000000   \n",
       "1       0.00000      0.00000      0.00000      0.00000            0.000000   \n",
       "2       0.00000      0.00000      0.00000      0.00000            0.000000   \n",
       "3       0.00000      0.00000      0.00000      0.00000            0.000000   \n",
       "4       0.00000      0.00000      0.00000      0.00000            0.000000   \n",
       "...         ...          ...          ...          ...                 ...   \n",
       "1546    0.00000      0.00000      0.00000      0.00000            0.000000   \n",
       "1547    0.00000      0.00000      0.00000      0.00000            0.000000   \n",
       "1548    0.31942      0.31942      0.31942      0.31942            0.000000   \n",
       "1549    0.00000      0.00000      0.00000      0.00000            0.667878   \n",
       "1550    0.00000      0.00000      0.00000      0.00000            0.000000   \n",
       "\n",
       "      lem_bemoaned  label_bias  \n",
       "0         0.000000           1  \n",
       "1         0.000000           0  \n",
       "2         0.000000           1  \n",
       "3         0.000000           0  \n",
       "4         0.000000           1  \n",
       "...            ...         ...  \n",
       "1546      0.000000           1  \n",
       "1547      0.000000           1  \n",
       "1548      0.000000           0  \n",
       "1549      0.000000           0  \n",
       "1550      0.734666           0  \n",
       "\n",
       "[1551 rows x 14394 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"origin_sentence\": dataset[\"origin_sentence\"]}\n",
    "\n",
    "for column in metaset_data:\n",
    "    data[\"meta_\"+column] = metaset_data[column]\n",
    "for column in stem_word_matrix_data:\n",
    "    data[\"stem_\"+column] = stem_word_matrix_data[column]\n",
    "print(len(data))\n",
    "for column in lem_word_matrix_data:\n",
    "    data[\"lem_\"+column] = lem_word_matrix_data[column]\n",
    "data['label_bias'] = dataset['label']\n",
    "\n",
    "big_data = pandas.DataFrame(data=data)\n",
    "\n",
    "dump(big_data, 'Joblibs/big_data.joblib')\n",
    "\n",
    "big_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6574e",
   "metadata": {},
   "source": [
    "### Implement voting classifier through pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cfd5896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "\n",
    "try:\n",
    "    big_data\n",
    "    dt\n",
    "    knns\n",
    "    gbdt_stem\n",
    "    gbdt_lem\n",
    "    mlp_stem\n",
    "    mlp_lem\n",
    "except NameError:\n",
    "    big_data = load('Joblibs/big_data.joblib')\n",
    "    dt = load('Joblibs/decision_trees.joblib')\n",
    "    knns = load('Joblibs/k-nearest_neighbors.joblib')\n",
    "    gbdt_stem = load('Joblibs/gradient_boosted_stem.joblib')\n",
    "    gbdt_lem = load('Joblibs/gradient_boosted_lem.joblib')\n",
    "    mlp_stem = load('Joblibs/multi_layer_perceptron_stem.joblib')\n",
    "    mlp_lem = load('Joblibs/multi_layer_perceptron_lem.joblib')\n",
    "\n",
    "classifiers = [make_pipeline(ColumnSelector(cols=(1, 3)), dt),\n",
    "               make_pipeline(ColumnSelector(cols=(1, 3)), knns),\n",
    "               make_pipeline(ColumnSelector(cols=(4, 6302)), gbdt_stem),\n",
    "               make_pipeline(ColumnSelector(cols=(4, 6302)), mlp_stem),\n",
    "               make_pipeline(ColumnSelector(cols=(6302, -1)), gbdt_lem),\n",
    "               make_pipeline(ColumnSelector(cols=(6302, -1)), mlp_lem)\n",
    "              ]\n",
    "voters = EnsembleVoteClassifier(clfs=classifiers, voting='soft', weights=[1.4, 1.4, 1, 0.2, 0.2, 1])\n",
    "\n",
    "scores = model_selection.cross_val_score(voters,\n",
    "                                         big_data.drop(['label_bias'], axis=1),\n",
    "                                         big_data['label_bias'],\n",
    "                                         cv=10,\n",
    "                                         scoring='accuracy')\n",
    "\n",
    "print(f\"Accuracy: %0.2f)\" % (scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36667acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data.to_csv('final_dataset.csv', index=False)\n",
    "lem_word_matrix.to_csv('lem_word.csv', index=False)\n",
    "stem_word_matrix.to_csv('stem_word.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
